{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_na_values(df):\n",
    "    \"\"\"\n",
    "    Replace a broad range of 'NA-like' string patterns, empty strings, and placeholders with pd.NA.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to clean.\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with standardized NA values.\n",
    "    \"\"\"\n",
    "    # Expanded patterns to recognize as NA\n",
    "    na_patterns = [\n",
    "        r\"^\\s*$\",               # empty or whitespace only\n",
    "        r\"^(n/?a)$\",            # NA, N/A, n/a, na\n",
    "        r\"^(none)$\",            # none\n",
    "        r\"^(null)$\",            # null\n",
    "        r\"^<na>$\",              # <NA>\n",
    "        r\"^#n/a$\",              # #N/A\n",
    "        r\"^\\[?na\\]?$\",          # [NA], NA\n",
    "        r\"^nan$\",               # nan, NaN\n",
    "        r\"^nil$\",               # nil\n",
    "        r\"^missing$\",           # missing\n",
    "        r\"^unknown$\",           # unknown\n",
    "        r\"^unavailable$\",       # unavailable\n",
    "        r\"^undisclosed$\",       # undisclosed\n",
    "        r\"^tbd$\",               # TBD\n",
    "        r\"^#value!$\",           # #VALUE!\n",
    "        r\"^\\?$\",                # ?\n",
    "        r\"^-+$\",                # -, --, ---\n",
    "        r\"^\\.\\.\\.$\",            # ...\n",
    "    ]\n",
    "    \n",
    "    # Combine patterns into a regex\n",
    "    na_regex = re.compile(\"|\".join(na_patterns), re.IGNORECASE)\n",
    "    \n",
    "    # Apply regex replacement to all string-like elements\n",
    "    return df.applymap(lambda x: pd.NA if isinstance(x, str) and na_regex.match(x.strip()) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id\n",
       "0       1\n",
       "803     1\n",
       "801     1\n",
       "800     1\n",
       "799     1\n",
       "       ..\n",
       "398     1\n",
       "397     1\n",
       "396     1\n",
       "395     1\n",
       "1195    1\n",
       "Name: count, Length: 1196, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: first_name\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "first_name\n",
       "alex       12\n",
       "michael    10\n",
       "emma        8\n",
       "matthew     8\n",
       "jack        7\n",
       "           ..\n",
       "riley       1\n",
       "kritika     1\n",
       "jinet       1\n",
       "amiya       1\n",
       "sebs        1\n",
       "Name: count, Length: 765, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: last_name\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "last_name\n",
       "wang              10\n",
       "lee                8\n",
       "kim                7\n",
       "li                 5\n",
       "cohen              5\n",
       "                  ..\n",
       "murigande          1\n",
       "jm                 1\n",
       "kalish-schur       1\n",
       "mikrogiannakis     1\n",
       "colegio            1\n",
       "Name: count, Length: 1025, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: gender\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gender\n",
       "NaN    658\n",
       "F      279\n",
       "M      259\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: class_year\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "class_year\n",
       "NaN     461\n",
       "2028    253\n",
       "2025    192\n",
       "2026    162\n",
       "2027    119\n",
       "<NA>      9\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: is_jewish\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "is_jewish\n",
       "NaN     605\n",
       "N       403\n",
       "J       185\n",
       "<NA>      3\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: school\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "school\n",
       "Harvard    525\n",
       "NaN        273\n",
       "MIT        254\n",
       "<NA>        83\n",
       "Other       61\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: preferred_name\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "preferred_name\n",
       "<NA>    1196\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "first_name           0\n",
       "last_name            0\n",
       "gender             658\n",
       "class_year         470\n",
       "is_jewish          608\n",
       "school             356\n",
       "preferred_name    1196\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final extracted emails preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>person_id</th>\n",
       "      <th>contact_type</th>\n",
       "      <th>contact_value</th>\n",
       "      <th>is_verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>school email</td>\n",
       "      <td>zakarias_erdos@college.harvard.edu</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>school email</td>\n",
       "      <td>Yehudahtor@college.harvard.edu</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>school email</td>\n",
       "      <td>zachabrams@college.harvard.edu</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>school email</td>\n",
       "      <td>smikhail@college.harvard.edu</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>school email</td>\n",
       "      <td>farukoztok@college.harvard.edu</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  person_id  contact_type                       contact_value  \\\n",
       "0   0          0  school email  zakarias_erdos@college.harvard.edu   \n",
       "1   1          1  school email      Yehudahtor@college.harvard.edu   \n",
       "2   2          2  school email      zachabrams@college.harvard.edu   \n",
       "3   3          3  school email        smikhail@college.harvard.edu   \n",
       "4   4          4  school email      farukoztok@college.harvard.edu   \n",
       "\n",
       "   is_verified  \n",
       "0        False  \n",
       "1        False  \n",
       "2        False  \n",
       "3        False  \n",
       "4        False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and clean main dataset\n",
    "base = pd.read_csv(\"Raw/all_people.csv\").replace(\"not harvard\", pd.NA)\n",
    "\n",
    "# Standardizing grades and names\n",
    "base[\"Grade\"] = (\n",
    "    base[\"Grade\"]\n",
    "    .replace({\"S\": \"Senior\", \"Other\": pd.NA, \"Graduate\": pd.NA, \"GRAD\": pd.NA})\n",
    "    .replace({\"Senior\": 2025, \"Junior\": 2026, \"Sophomore\": 2027, \"Freshman\": 2028})\n",
    ")\n",
    "base[\"J/N\"] = base[\"J/N\"].replace(\"F\", pd.NA)\n",
    "base[\"First Name\"] = base[\"first_name\"].str.strip().str.lower()\n",
    "base[\"Last Name\"] = base[\"last_name\"].str.strip().str.lower()\n",
    "base = base.dropna(subset=[\"last_name\"]).reset_index(drop=True)\n",
    "\n",
    "# Assuming you have a `clean_na_values` function\n",
    "base = clean_na_values(base)\n",
    "base[\"id\"] = base.index\n",
    "\n",
    "# ------------------- NEW EMAIL EXTRACTION LOGIC -------------------\n",
    "\n",
    "# Function to extract emails based on domain pattern\n",
    "def extract_emails(df, email_columns, edu=True):\n",
    "    email_list = []\n",
    "    for col in email_columns:\n",
    "        temp = df[[\"id\", col]].dropna(subset=[col]).copy()\n",
    "        if edu:\n",
    "            temp = temp[temp[col].str.endswith(\".edu\", na=False)]\n",
    "        else:\n",
    "            temp = temp[~temp[col].str.endswith(\".edu\", na=False)]\n",
    "        temp[\"contact_type\"] = \"school email\" if edu else \"personal email\"\n",
    "        temp.columns = [\"person_id\", \"contact_value\", \"contact_type\"]\n",
    "        email_list.append(temp)\n",
    "    # Concatenate and drop duplicates\n",
    "    result = pd.concat(email_list).drop_duplicates(subset=[\"person_id\", \"contact_value\"]).reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "# Extract school emails (.edu) and personal emails (non-.edu) from both columns\n",
    "school_emails = extract_emails(base, [\"School Email\", \"Emails\"], edu=True)\n",
    "personal_emails = extract_emails(base, [\"School Email\", \"Emails\"], edu=False)\n",
    "\n",
    "# Combine all emails, deduplicate on same contact_value per person\n",
    "all_emails = pd.concat([school_emails, personal_emails])\n",
    "all_emails = all_emails.drop_duplicates(subset=[\"person_id\", \"contact_value\"]).reset_index(drop=True)\n",
    "all_emails[\"id\"] = all_emails.index\n",
    "all_emails[\"is_verified\"] = False\n",
    "all_emails = all_emails[[\"id\", \"person_id\", \"contact_type\", \"contact_value\", \"is_verified\"]]\n",
    "\n",
    "# ------------------- CLEAN FINAL PEOPLE BASE -------------------\n",
    "\n",
    "base = base[[\"id\", \"First Name\", \"Last Name\", \"Gender\", \"Grade\", \"J/N\", \"School\"]]\n",
    "base.columns = [\"id\", \"first_name\", \"last_name\", \"gender\", \"class_year\", \"is_jewish\", \"school\"]\n",
    "base[\"preferred_name\"] = pd.NA\n",
    "base = base.reset_index(drop=True)\n",
    "\n",
    "# ------------------- DEBUGGING OUTPUT -------------------\n",
    "\n",
    "for col in base.columns:\n",
    "    print(f\"Column: {col}\")\n",
    "    display(base[col].value_counts(dropna=False))\n",
    "\n",
    "print(\"Missing values summary:\")\n",
    "display(base.isna().sum())\n",
    "\n",
    "# Preview of final emails\n",
    "print(\"Final extracted emails preview:\")\n",
    "display(all_emails.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "teds_id = base[(base[\"first_name\"] == \"theodore\") & (base[\"last_name\"] == \"sunshine\")][\"id\"].values[0]\n",
    "all_emails.loc[len(all_emails)] = [len(all_emails), teds_id, \"personal email\", \"tedsunshine@gmail.com\", False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>person_id</th>\n",
       "      <th>contact_type</th>\n",
       "      <th>contact_value</th>\n",
       "      <th>is_verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>85</td>\n",
       "      <td>school email</td>\n",
       "      <td>theodore_sunshine@college.harvard.edu</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>722</td>\n",
       "      <td>85</td>\n",
       "      <td>personal email</td>\n",
       "      <td>teddysunshine@icloud.com</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>1205</td>\n",
       "      <td>85</td>\n",
       "      <td>personal email</td>\n",
       "      <td>tedsunshine@gmail.com</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  person_id    contact_type                          contact_value  \\\n",
       "80      80         85    school email  theodore_sunshine@college.harvard.edu   \n",
       "722    722         85  personal email               teddysunshine@icloud.com   \n",
       "1205  1205         85  personal email                  tedsunshine@gmail.com   \n",
       "\n",
       "      is_verified  \n",
       "80          False  \n",
       "722         False  \n",
       "1205        False  "
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_emails[all_emails[\"person_id\"] == 85]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.to_csv(\"final/people.csv\", index=False)\n",
    "all_emails.to_csv(\"final/contacts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_data = [\n",
    "    [\"prelaunch\", \"speaker\", \"sunshine residence\", \"September 26, 2024, 5:30pm EST\", \"Jonathan Kraft spoke to students at the Sunshine residence\"],\n",
    "    [\"launch\", \"party\", \"prudential center\", \"September 26, 2024, 7:30pm EST\", \"Party at the top of the prudential center\"],\n",
    "    [\"russell house\", \"speaker dinner\", \"russell house\", \"October 14, 2024, 7:30pm EST\" , \"Join us for our first official event this coming Monday, October 14th at Russell House Tavern from 7:30 - 9:30 with dinner and of course an open bar. You’ll meet and chat with several Israeli tech founders. Our featured guests include the co-founder and CTO of a cloud security company acquired by Cisco, as well as a tech innovator who has developed an app that uses augmented reality to make art and creativity more accessible.\"],\n",
    "    [\"viale\", \"speaker dinner\", \"viale\", \"October 30, 2024, 7:45pm EST\", \"We’re excited to invite you to the next event hosted by The Camel at Viale, Central Square. As always, drinks and food are on us! Join us for a conversation with Boaz Fachler, Principal at Link Ventures, Dror Zaide, Co-Founder at Eleos Health, and a surprise guest (Adam Valkin), followed by a cocktail reception and dinner.\"],\n",
    "    [\"sababa nights\", \"party\", \"Dx\", \"November 7, 2024, 10:00pm EST\", \"Disco Rave this Thursday! Harvard x MIT party at the DX hosted by the Camel. Open bar and live DJ from Berklee. Bring your friends!\" ],\n",
    "    [\"amplify\", \"speaker dinner\", \"bar enza\", \"November 13, 2024, 7:00pm EST\", \"Amplify VC hosted/funded dinner at bar enza\"],\n",
    "    [\"community dinners I\", \"community dinner\", \"various\", \"November 20, 2024, 7:00pm EST\", \"Community dinners hosted by Camel, monday, wednesday, and thursday options\"],\n",
    "    [\"secret sip\", \"community dinner\", \"sunshine residence\", \"December 6, 2024, 6:00pm EST\", \"Join us for a Shabbat dinner and afterparty with an intimate group of Harvard and MIT Camel community, just before we wrap up the semester.\"],\n",
    "    [\"pentera\", \"speaker dinner\", \"daedalus\", \"February 5, 2025, 7:00pm EST\", \"Hi Everyone! This coming Wednesday, Amitai Ratzon will be joining us at Bar Enza at 7 pm. Amitai is the CEO of Pentera, a cybersecurity unicorn. As always, there will be open bar and dinner.\"],\n",
    "    [\"bsmnt\", \"party\", \"bsmnt\", \"February 6, 2025, 9:00pm EST\", \"There’s a Camel in the BSMNT. We don’t know how it got there. But it’s throwing a rave. We want to welcome you all back to school with a night at BSMNT—Boston’s newest nightclub. As always, there will be open bar all night, and DJ Costa will be on deck.\"],\n",
    "    [\"zelnick speaker\", \"speaker\", \"sheraton commander\", \"February 13, 2025, 6:00pm EST\", \"This Thursday (February 13th) at 6 pm, we are bringing Strauss Zelnick to Sheraton Commander for an exclusive evening of drinks and conversation. Strauss is the CEO of Take-Two Interactive — the creators of NBA 2K and GTA — and the former chairman of the media giant CBS Corporation. He will be joining us to share insights on his career in Private Equity and Entertainment, and maybe even why GTA VI still hasn’t been released… See you soon!\"],\n",
    "    [\"zelnick dinner\", \"speaker dinner\", \"gufo\", \"February 13, 2025, 7:30pm EST\", \"Hi, We’re hosting a private, invite-only dinner with Strauss Zelnick, CEO of Take-Two Interactive (NBA 2K and GTA) and former chairman of CBS Corporation, and we’d love to have you there!\" ],\n",
    "    [\"community dinners II\", \"community dinner\", \"various\", \"February 26, 2025, 7:30pm EST\", \"Community dinners hosted by Camel\"],\n",
    "]\n",
    "\n",
    "events = pd.DataFrame(events_data, columns = [\"event_name\", \"category\", \"location\", \"start_datetime\", \"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event_name</th>\n",
       "      <th>category</th>\n",
       "      <th>location</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>prelaunch</td>\n",
       "      <td>speaker</td>\n",
       "      <td>sunshine residence</td>\n",
       "      <td>2024-09-26 17:30:00-05:00</td>\n",
       "      <td>Jonathan Kraft spoke to students at the Sunshi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>launch</td>\n",
       "      <td>party</td>\n",
       "      <td>prudential center</td>\n",
       "      <td>2024-09-26 19:30:00-05:00</td>\n",
       "      <td>Party at the top of the prudential center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>russell house</td>\n",
       "      <td>speaker dinner</td>\n",
       "      <td>russell house</td>\n",
       "      <td>2024-10-14 19:30:00-05:00</td>\n",
       "      <td>Join us for our first official event this comi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>viale</td>\n",
       "      <td>speaker dinner</td>\n",
       "      <td>viale</td>\n",
       "      <td>2024-10-30 19:45:00-05:00</td>\n",
       "      <td>We’re excited to invite you to the next event ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sababa nights</td>\n",
       "      <td>party</td>\n",
       "      <td>Dx</td>\n",
       "      <td>2024-11-07 22:00:00-05:00</td>\n",
       "      <td>Disco Rave this Thursday! Harvard x MIT party ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>amplify</td>\n",
       "      <td>speaker dinner</td>\n",
       "      <td>bar enza</td>\n",
       "      <td>2024-11-13 19:00:00-05:00</td>\n",
       "      <td>Amplify VC hosted/funded dinner at bar enza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>community dinners I</td>\n",
       "      <td>community dinner</td>\n",
       "      <td>various</td>\n",
       "      <td>2024-11-20 19:00:00-05:00</td>\n",
       "      <td>Community dinners hosted by Camel, monday, wed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>secret sip</td>\n",
       "      <td>community dinner</td>\n",
       "      <td>sunshine residence</td>\n",
       "      <td>2024-12-06 18:00:00-05:00</td>\n",
       "      <td>Join us for a Shabbat dinner and afterparty wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>pentera</td>\n",
       "      <td>speaker dinner</td>\n",
       "      <td>daedalus</td>\n",
       "      <td>2025-02-05 19:00:00-05:00</td>\n",
       "      <td>Hi Everyone! This coming Wednesday, Amitai Rat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>bsmnt</td>\n",
       "      <td>party</td>\n",
       "      <td>bsmnt</td>\n",
       "      <td>2025-02-06 21:00:00-05:00</td>\n",
       "      <td>There’s a Camel in the BSMNT. We don’t know ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>zelnick speaker</td>\n",
       "      <td>speaker</td>\n",
       "      <td>sheraton commander</td>\n",
       "      <td>2025-02-13 18:00:00-05:00</td>\n",
       "      <td>This Thursday (February 13th) at 6 pm, we are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>zelnick dinner</td>\n",
       "      <td>speaker dinner</td>\n",
       "      <td>gufo</td>\n",
       "      <td>2025-02-13 19:30:00-05:00</td>\n",
       "      <td>Hi, We’re hosting a private, invite-only dinne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>community dinners II</td>\n",
       "      <td>community dinner</td>\n",
       "      <td>various</td>\n",
       "      <td>2025-02-26 19:30:00-05:00</td>\n",
       "      <td>Community dinners hosted by Camel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id            event_name          category            location  \\\n",
       "0    0             prelaunch           speaker  sunshine residence   \n",
       "1    1                launch             party   prudential center   \n",
       "2    2         russell house    speaker dinner       russell house   \n",
       "3    3                 viale    speaker dinner               viale   \n",
       "4    4         sababa nights             party                  Dx   \n",
       "5    5               amplify    speaker dinner            bar enza   \n",
       "6    6   community dinners I  community dinner             various   \n",
       "7    7            secret sip  community dinner  sunshine residence   \n",
       "8    8               pentera    speaker dinner            daedalus   \n",
       "9    9                 bsmnt             party               bsmnt   \n",
       "10  10       zelnick speaker           speaker  sheraton commander   \n",
       "11  11        zelnick dinner    speaker dinner                gufo   \n",
       "12  12  community dinners II  community dinner             various   \n",
       "\n",
       "              start_datetime  \\\n",
       "0  2024-09-26 17:30:00-05:00   \n",
       "1  2024-09-26 19:30:00-05:00   \n",
       "2  2024-10-14 19:30:00-05:00   \n",
       "3  2024-10-30 19:45:00-05:00   \n",
       "4  2024-11-07 22:00:00-05:00   \n",
       "5  2024-11-13 19:00:00-05:00   \n",
       "6  2024-11-20 19:00:00-05:00   \n",
       "7  2024-12-06 18:00:00-05:00   \n",
       "8  2025-02-05 19:00:00-05:00   \n",
       "9  2025-02-06 21:00:00-05:00   \n",
       "10 2025-02-13 18:00:00-05:00   \n",
       "11 2025-02-13 19:30:00-05:00   \n",
       "12 2025-02-26 19:30:00-05:00   \n",
       "\n",
       "                                          description  \n",
       "0   Jonathan Kraft spoke to students at the Sunshi...  \n",
       "1           Party at the top of the prudential center  \n",
       "2   Join us for our first official event this comi...  \n",
       "3   We’re excited to invite you to the next event ...  \n",
       "4   Disco Rave this Thursday! Harvard x MIT party ...  \n",
       "5         Amplify VC hosted/funded dinner at bar enza  \n",
       "6   Community dinners hosted by Camel, monday, wed...  \n",
       "7   Join us for a Shabbat dinner and afterparty wi...  \n",
       "8   Hi Everyone! This coming Wednesday, Amitai Rat...  \n",
       "9   There’s a Camel in the BSMNT. We don’t know ho...  \n",
       "10  This Thursday (February 13th) at 6 pm, we are ...  \n",
       "11  Hi, We’re hosting a private, invite-only dinne...  \n",
       "12                  Community dinners hosted by Camel  "
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[\"start_datetime\"] = pd.to_datetime(events[\"start_datetime\"], utc=False, format=\"%B %d, %Y, %I:%M%p %Z\", errors=\"coerce\")\n",
    "events.sort_values(by=\"start_datetime\", inplace=True)\n",
    "events.reset_index(drop=True, inplace=True)\n",
    "events[\"id\"] = events.index\n",
    "events=events[[\"id\", \"event_name\", \"category\", \"location\", \"start_datetime\", \"description\"]]\n",
    "events.to_csv(\"final/events.csv\", index=False)\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def fuzzy_ratio(str_a, str_b):\n",
    "    return SequenceMatcher(None, str_a, str_b).ratio()\n",
    "\n",
    "def is_initial(name):\n",
    "    cleaned = name.strip().lower()\n",
    "    if len(cleaned) == 1 and cleaned.isalpha():\n",
    "        return True\n",
    "    if len(cleaned) == 2 and cleaned[0].isalpha() and cleaned[1] == \".\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def compare_names(fn_ta, ln_ta, fn_sheet, ln_sheet, fuzzy_threshold=0.80):\n",
    "    fn_ta = fn_ta.strip().lower()\n",
    "    ln_ta = ln_ta.strip().lower()\n",
    "    fn_sheet = fn_sheet.strip().lower()\n",
    "    ln_sheet = ln_sheet.strip().lower()\n",
    "\n",
    "    # ===== STEP 1: Check if both first and last names are exact match =====\n",
    "    if fn_ta == fn_sheet and ln_ta == ln_sheet:\n",
    "        return \"auto_accept\"\n",
    "    # if one name is exact match and the other is a substring\n",
    "    elif (fn_ta in fn_sheet and ln_sheet == ln_ta) or (fn_sheet in fn_ta and ln_ta == ln_sheet):\n",
    "        return \"auto_accept\"\n",
    "    elif (fn_ta == fn_sheet and ln_ta in ln_sheet) or (fn_sheet == fn_ta and ln_sheet in ln_ta):\n",
    "        return \"auto_accept\"\n",
    "\n",
    "    # ===== STEP 2: Handle initials only when necessary =====\n",
    "    fn_ta_is_initial = is_initial(fn_ta)\n",
    "    ln_ta_is_initial = is_initial(ln_ta)\n",
    "\n",
    "    if fn_ta_is_initial or ln_ta_is_initial:\n",
    "        # Initial matching logic:\n",
    "        if fn_ta_is_initial:\n",
    "            letter = fn_ta[0].lower()\n",
    "            if not fn_sheet.startswith(letter):\n",
    "                return \"reject_now\"\n",
    "        if ln_ta_is_initial:\n",
    "            letter = ln_ta[0].lower()\n",
    "            if not ln_sheet.startswith(letter):\n",
    "                return \"reject_now\"\n",
    "\n",
    "        # If one of them is initial, require a good fuzzy match on the other\n",
    "        if fn_ta_is_initial and not ln_ta_is_initial:\n",
    "            ratio_last = fuzzy_ratio(ln_ta, ln_sheet)\n",
    "            if ratio_last >= fuzzy_threshold:\n",
    "                return \"manual_review\"\n",
    "            else:\n",
    "                return \"reject_now\"\n",
    "        elif ln_ta_is_initial and not fn_ta_is_initial:\n",
    "            ratio_first = fuzzy_ratio(fn_ta, fn_sheet)\n",
    "            if ratio_first >= fuzzy_threshold:\n",
    "                return \"manual_review\"\n",
    "            else:\n",
    "                return \"reject_now\"\n",
    "        else:\n",
    "            # Both initials that passed .startswith checks but aren't exact match\n",
    "            return \"manual_review\"\n",
    "\n",
    "    # ===== STEP 3: Fuzzy logic =====\n",
    "    exact_first = (fn_ta == fn_sheet)\n",
    "    exact_last = (ln_ta == ln_sheet)\n",
    "    ratio_first = fuzzy_ratio(fn_ta, fn_sheet)\n",
    "    ratio_last = fuzzy_ratio(ln_ta, ln_sheet)\n",
    "\n",
    "    # If one name is exact, and the other is a good fuzzy match\n",
    "    if exact_first and ratio_last >= fuzzy_threshold:\n",
    "        print(f\"Matching {fn_ta} {ln_ta} to {fn_sheet} {ln_sheet}\")\n",
    "        return \"auto_accept\"\n",
    "    if exact_last and ratio_first >= fuzzy_threshold:\n",
    "        print(f\"Matching {fn_ta} {ln_ta} to {fn_sheet} {ln_sheet}\")\n",
    "        return \"auto_accept\"\n",
    "\n",
    "    # If both are good fuzzy matches (but not exact)\n",
    "    if ratio_first >= fuzzy_threshold and ratio_last >= fuzzy_threshold:\n",
    "        return \"manual_review\"\n",
    "    # Otherwise, reject\n",
    "    return \"reject_now\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_names_if_substring(df, idx, sheet_first, sheet_last, input_first, input_last):\n",
    "    \"\"\"\n",
    "    Update the first_name and last_name in df at idx to the longer version\n",
    "    between input and sheet names if one is a substring of the other.\n",
    "    \"\"\"\n",
    "    # First name check\n",
    "    if sheet_first.lower() in input_first.lower() or input_first.lower() in sheet_first.lower():\n",
    "        longer_first = max(sheet_first, input_first, key=len)\n",
    "        df.at[idx, 'first_name'] = longer_first\n",
    "    \n",
    "    # Last name check (if both are provided)\n",
    "    if pd.notna(sheet_last) and pd.notna(input_last):\n",
    "        if sheet_last.lower() in input_last.lower() or input_last.lower() in sheet_last.lower():\n",
    "            longer_last = max(sheet_last, input_last, key=len)\n",
    "            df.at[idx, 'last_name'] = longer_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_person_id(\n",
    "    row,\n",
    "    people_df,\n",
    "    contacts_df,\n",
    "    email_col=None,\n",
    "    handle_indices_list=None,\n",
    "    fuzzy_threshold=0.80,  # Passed to compare_names\n",
    "):\n",
    "    if pd.isna(row[\"first_name\"]):\n",
    "        raise ValueError(\"First name is missing in the row.\")\n",
    "\n",
    "    first_name = row[\"first_name\"].strip().lower()\n",
    "    last_name = row.get(\"last_name\")\n",
    "    last_name = last_name.strip().lower() if pd.notna(last_name) else None\n",
    "\n",
    "    # 1) Email matching\n",
    "    if email_col and email_col in row and pd.notna(row[email_col]):\n",
    "        email = str(row[email_col]).strip().lower()\n",
    "        email_matches = contacts_df[contacts_df[\"contact_value\"].str.lower() == email]\n",
    "        if not email_matches.empty:\n",
    "            person_id = email_matches.iloc[0][\"person_id\"]\n",
    "            idx = people_df[people_df[\"id\"] == person_id].index[0]  # map to index\n",
    "            update_names_if_substring(people_df, idx, people_df.loc[idx, \"first_name\"], people_df.loc[idx, \"last_name\"], first_name, last_name)\n",
    "            return person_id, people_df\n",
    "        print(f\"Could not find person with email: {email}\")\n",
    "        handle_indices_list.append((first_name, last_name))\n",
    "    else:\n",
    "        email = None\n",
    "\n",
    "    # 2) Exact name matching\n",
    "    potentials = people_df[\n",
    "        (people_df[\"first_name\"].str.lower() == first_name) &\n",
    "        ((people_df[\"last_name\"].str.lower() == last_name) if last_name else True)\n",
    "    ]\n",
    "\n",
    "    if len(potentials) == 1:\n",
    "        idx = potentials.index[0]\n",
    "        update_names_if_substring(people_df, idx, potentials.loc[idx, \"first_name\"], potentials.loc[idx, \"last_name\"], first_name, last_name)\n",
    "        return potentials.loc[idx, \"id\"], people_df\n",
    "\n",
    "    elif len(potentials) > 1:\n",
    "        options = [\n",
    "            f\"{i} => {p['first_name']} {p['last_name']} (gender={p['gender']}, jewish={p['is_jewish']})\"\n",
    "            for i, (_, p) in enumerate(potentials.iterrows())\n",
    "        ]\n",
    "        options_str = \"\\n\".join(options)\n",
    "        choice = input(f\"Multiple exact matches for '{first_name} {last_name or ''} and email {email}'. Choose one:\\n\\n{options_str}\\n\\nSelect index or 'n' to skip: \")\n",
    "        if choice.lower() == \"n\":\n",
    "            handle_indices_list.append((first_name, last_name))\n",
    "            return None, people_df\n",
    "        try:\n",
    "            selected_idx = potentials.index[int(choice)]\n",
    "            person_id = people_df.loc[selected_idx, \"id\"]\n",
    "            update_names_if_substring(people_df, selected_idx, people_df.loc[selected_idx, \"first_name\"], people_df.loc[selected_idx, \"last_name\"], first_name, last_name)\n",
    "            return person_id, people_df\n",
    "        except:\n",
    "            print(\"Invalid choice. Skipping.\")\n",
    "            handle_indices_list.append((first_name, last_name))\n",
    "            return None, people_df\n",
    "\n",
    "    # 3) Fuzzy matching\n",
    "    auto_accepts, manual_reviews = [], []\n",
    "    for idx, candidate in people_df.iterrows():\n",
    "        verdict = compare_names(\n",
    "            first_name,\n",
    "            (last_name or \"\"),\n",
    "            candidate[\"first_name\"],\n",
    "            candidate[\"last_name\"] if pd.notna(candidate[\"last_name\"]) else \"\",\n",
    "            fuzzy_threshold,\n",
    "        )\n",
    "        if verdict == \"auto_accept\":\n",
    "            auto_accepts.append(idx)\n",
    "        elif verdict == \"manual_review\":\n",
    "            manual_reviews.append(idx)\n",
    "\n",
    "    # Handle auto-accept\n",
    "    if len(auto_accepts) == 1:\n",
    "        idx = auto_accepts[0]\n",
    "        update_names_if_substring(people_df, idx, people_df.loc[idx, \"first_name\"], people_df.loc[idx, \"last_name\"], first_name, last_name)\n",
    "        return people_df.loc[idx, \"id\"], people_df\n",
    "\n",
    "    if len(auto_accepts) > 1:\n",
    "        options = [\n",
    "            f\"{i} => {people_df.loc[idx, 'first_name']} {people_df.loc[idx, 'last_name']} (gender={people_df.loc[idx, 'gender']}, jewish={people_df.loc[idx, 'is_jewish']})\"\n",
    "            for i, idx in enumerate(auto_accepts)\n",
    "        ]\n",
    "        options_str = \"\\n\".join(options)\n",
    "        choice = input(f\"Multiple 'auto_accept' matches for '{first_name} {last_name or ''} and email {email}'. Choose one:\\n\\n{options_str}\\n\\nSelect index or 'n' to skip: \")\n",
    "        if choice.lower() == \"n\":\n",
    "            handle_indices_list.append((first_name, last_name))\n",
    "            return None, people_df\n",
    "        try:\n",
    "            idx = auto_accepts[int(choice)]\n",
    "            update_names_if_substring(people_df, idx, people_df.loc[idx, \"first_name\"], people_df.loc[idx, \"last_name\"], first_name, last_name)\n",
    "            return people_df.loc[idx, \"id\"], people_df\n",
    "        except:\n",
    "            print(\"Invalid choice. Skipping.\")\n",
    "            handle_indices_list.append((first_name, last_name))\n",
    "            return None, people_df\n",
    "\n",
    "    # Manual review\n",
    "    if manual_reviews:\n",
    "        options = [\n",
    "            f\"{i} => {people_df.loc[idx, 'first_name']} {people_df.loc[idx, 'last_name']} (gender={people_df.loc[idx, 'gender']}, jewish={people_df.loc[idx, 'is_jewish']})\"\n",
    "            for i, idx in enumerate(manual_reviews)\n",
    "        ]\n",
    "        options_str = \"\\n\".join(options)\n",
    "        choice = input(f\"No auto-accept found for '{first_name} {last_name or ''}', but possible matches:\\n\\n{options_str}\\n\\nSelect index or 'n' to skip: \")\n",
    "        if choice.lower() == \"n\":\n",
    "            handle_indices_list.append((first_name, last_name))\n",
    "            return None, people_df\n",
    "        try:\n",
    "            idx = manual_reviews[int(choice)]\n",
    "            update_names_if_substring(people_df, idx, people_df.loc[idx, \"first_name\"], people_df.loc[idx, \"last_name\"], first_name, last_name)\n",
    "            return people_df.loc[idx, \"id\"], people_df\n",
    "        except:\n",
    "            print(\"Invalid choice. Skipping.\")\n",
    "            handle_indices_list.append((first_name, last_name))\n",
    "            return None, people_df\n",
    "\n",
    "    # No matches\n",
    "    print(f\"No match found for '{first_name} {last_name or ''}'.\")\n",
    "    handle_indices_list.append((first_name, last_name))\n",
    "    return None, people_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 attended sheets and 10 rsvp sheets, total 23\n",
      "Handling RSVPs for Pentera_Posh.csv...\n",
      "\n",
      "Handling RSVPs for SababaNights_Partiful.csv...\n",
      "\n",
      "No match found for ' '.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/8xv4dl8n2rv9z7xy2ms9hy700000gn/T/ipykernel_64221/640129933.py:68: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  rsvp_csv[rsvp_date_col] = pd.to_datetime(rsvp_csv[rsvp_date_col], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match found for 'courtney '.\n",
      "No match found for 'dreese '.\n",
      "No match found for 'e '.\n",
      "No match found for 'eliska '.\n",
      "No match found for 'fernanda '.\n",
      "No match found for 'francisco '.\n",
      "No match found for 'gabe evers'.\n",
      "No match found for 'johnson '.\n",
      "No match found for 'kami '.\n",
      "No match found for 'kiran '.\n",
      "Matching luke huang to luke chung\n",
      "No match found for 'nitish '.\n",
      "No match found for 'nolan '.\n",
      "No match found for 'prash '.\n",
      "No match found for 'samay '.\n",
      "No match found for 'saphina '.\n",
      "Matching simona letizia to simone letizia\n",
      "No match found for 'sreela '.\n",
      "Handling RSVPs for ZelnickSpeaker_Posh.csv...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/8xv4dl8n2rv9z7xy2ms9hy700000gn/T/ipykernel_64221/640129933.py:68: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  rsvp_csv[rsvp_date_col] = pd.to_datetime(rsvp_csv[rsvp_date_col], errors=\"coerce\")\n",
      "/var/folders/w_/8xv4dl8n2rv9z7xy2ms9hy700000gn/T/ipykernel_64221/640129933.py:68: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  rsvp_csv[rsvp_date_col] = pd.to_datetime(rsvp_csv[rsvp_date_col], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling RSVPs for ZelnickDinner_Posh.csv...\n",
      "\n",
      "Handling RSVPs for Launch_Partiful.csv...\n",
      "\n",
      "Matching dorde ivanovic to djordje ivanovic\n",
      "Matching frederico araujo to frederico araújo\n",
      "No match found for 'j. desire abayo'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/8xv4dl8n2rv9z7xy2ms9hy700000gn/T/ipykernel_64221/640129933.py:68: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  rsvp_csv[rsvp_date_col] = pd.to_datetime(rsvp_csv[rsvp_date_col], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling RSVPs for BSMNT_Posh.csv...\n",
      "\n",
      "Could not find person with email: 69@hotmail.com\n",
      "Could not find person with email: okeynwaishienyi@gmail.com\n",
      "No match found for ' '.\n",
      "Could not find person with email: woodbridge.harvard@gmail.com\n",
      "Handling RSVPs for PreLaunch_Partiful.csv...\n",
      "\n",
      "Handling RSVPs for Viale_Partiful.csv...\n",
      "\n",
      "No match found for 'dreese '.\n",
      "Handling RSVPs for Russell_Partiful.csv...\n",
      "\n",
      "Handling RSVPs for SecretSip_Partiful.csv...\n",
      "\n",
      "No match found for 'giovanni maria d'antonio'.\n",
      "Handling attendance for Viale_Sheets.csv...\n",
      "\n",
      "Handling attendance for PreLaunch_Sheets.csv...\n",
      "\n",
      "Handling attendance for ZelnickSpeaker_Sheets.csv...\n",
      "\n",
      "Matching ashey chamoy to asher chamoy\n",
      "No match found for 'anil candilcor'.\n",
      "Matching matthew herschfeld to matthew hirschfeld\n",
      "Matching alexander otto to alexandra otto\n",
      "Matching adriano ariota to adriano arioto\n",
      "No match found for 'cybele '.\n",
      "Matching yagnur kavukcuoglu to yagmur kavukcuoglu\n",
      "Handling attendance for SababaNights_Sheets.csv...\n",
      "\n",
      "No match found for 'clyde '.\n",
      "No match found for 'n '.\n",
      "No match found for 'ruby '.\n",
      "No match found for 'xhech '.\n",
      "No match found for 'bemnet '.\n",
      "No match found for 'gabe evers'.\n",
      "No match found for 'maggie '.\n",
      "No match found for 'meera '.\n",
      "No match found for 'adi '.\n",
      "No match found for 'elior '.\n",
      "No match found for 'el '.\n",
      "No match found for 'dari '.\n",
      "Handling attendance for SecretSip_Sheets.csv...\n",
      "\n",
      "Handling attendance for ZelnickDinner_Sheets.csv...\n",
      "\n",
      "No match found for 'ted '.\n",
      "No match found for 'adrinao '.\n",
      "No match found for 'cybele '.\n",
      "Handling attendance for Russell_Sheets.csv...\n",
      "\n",
      "Handling attendance for Pentera_Sheets.csv...\n",
      "\n",
      "Matching lindsay brail to lindsey brail\n",
      "Handling attendance for Launch_Sheets.csv...\n",
      "\n",
      "Handling no RSVPs for Amplify_Sheets.csv...\n",
      "\n",
      "Matching ana tejada to ana tejeda\n",
      "Handling no RSVPs for Community Dinners II_Sheets.csv...\n",
      "\n",
      "Matching maría santos to maria santos\n",
      "Matching tara razaei to tara rezaei\n",
      "Handling no RSVPs for CommunityDinners_Sheets.csv...\n",
      "\n",
      "Found 144 attendees with no RSVP.\n",
      "Made 2354 attendance entries.\n",
      "Found 41 rsvp indices to handle.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "people = pd.read_csv(\"final/people.csv\")\n",
    "people['class_year'] = people['class_year']\n",
    "contacts = pd.read_csv(\"final/contacts.csv\")\n",
    "events = pd.read_csv(\"final/events.csv\")\n",
    "\n",
    "to_handle = {}\n",
    "\n",
    "attended_sheets = [f for f in os.listdir(\"Raw\") if \"Sheets\" in f]\n",
    "no_rsvp_sheets = [\"Amplify_Sheets.csv\", \"Community Dinners II_Sheets.csv\", \"CommunityDinners_Sheets.csv\"]\n",
    "attended_sheets = [f for f in attended_sheets if f not in no_rsvp_sheets]\n",
    "rsvp_sheets = [f for f in os.listdir(\"Raw\") if \"Posh\" in f or \"Partiful\" in f]\n",
    "\n",
    "print(f\"Found {len(attended_sheets)} attended sheets and {len(rsvp_sheets)} rsvp sheets, total {len([f for f in os.listdir(\"Raw\") if f.endswith(\".csv\")])}\")\n",
    "# Create final attendance DF\n",
    "attendance = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"person_id\",\n",
    "        \"event_id\",\n",
    "        \"rsvp\",\n",
    "        \"approved\",\n",
    "        \"checked_in\",\n",
    "        \"rsvp_datetime\",\n",
    "        \"is_first_event\",\n",
    "        \"invite_token_id\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# rsvp params\n",
    "###########################################################\n",
    "eid_key = {\n",
    "    \"Launch_Partiful.csv\": 1,\n",
    "    \"Pentera_Posh.csv\": 8,\n",
    "    \"PreLaunch_Partiful.csv\": 0,\n",
    "    \"Russell_Partiful.csv\": 2,\n",
    "    \"SababaNights_Partiful.csv\": 4,\n",
    "    \"SecretSip_Partiful.csv\": 7,\n",
    "    \"Viale_Partiful.csv\": 3,\n",
    "    \"ZelnickDinner_Posh.csv\": 11,\n",
    "    \"ZelnickSpeaker_Posh.csv\": 10,\n",
    "    \"BSMNT_Posh.csv\": 9,\n",
    "}\n",
    "indices_to_handle_rsvp = []\n",
    "for rsvp_csv_file_name in rsvp_sheets:\n",
    "    eid = eid_key[rsvp_csv_file_name]\n",
    "    rsvp_date_col = \"rsvp_date\" if \"partiful\" in rsvp_csv_file_name.lower() else \"order_date\"\n",
    "    approved_col = \"approved\"\n",
    "\n",
    "    # Read CSVs\n",
    "    rsvp_csv = pd.read_csv(f\"Raw/{rsvp_csv_file_name}\")\n",
    "    # Split the 'name' column into 'first_name' and 'last_name'\n",
    "    rsvp_csv[[\"first_name\", \"last_name\"]] = (\n",
    "        rsvp_csv[\"name\"].fillna(\"\").str.split(\" \", n=1, expand=True)\n",
    "    )\n",
    "\n",
    "    rsvp_email_col = \"email\" if \"email\" in rsvp_csv.columns else None\n",
    "\n",
    "    # Convert RSVP date columns to datetime with a timezone\n",
    "    rsvp_csv[rsvp_date_col] = pd.to_datetime(rsvp_csv[rsvp_date_col], errors=\"coerce\")\n",
    "    rsvp_csv[rsvp_date_col] = rsvp_csv[rsvp_date_col].dt.tz_localize(\n",
    "        \"America/New_York\", ambiguous=\"NaT\", nonexistent=\"NaT\"\n",
    "    )\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Process RSVPs\n",
    "    # -----------------------------------------------------------------------------\n",
    "    print(f\"Handling RSVPs for {rsvp_csv_file_name}...\\n\")\n",
    "    for i, rsvp_row in rsvp_csv.iterrows():\n",
    "        pid, people = find_person_id(\n",
    "            rsvp_row,\n",
    "            people,\n",
    "            contacts,\n",
    "            email_col=rsvp_email_col,\n",
    "            handle_indices_list=indices_to_handle_rsvp,\n",
    "        )\n",
    "        if pid is not None:\n",
    "            if rsvp_csv_file_name == \"BSMNT_Posh.csv\":\n",
    "                checked_in = rsvp_row[\"scanned\"] == 1\n",
    "            else:\n",
    "                checked_in = False\n",
    "            attendance.loc[len(attendance)] = [\n",
    "                pid,\n",
    "                eid,\n",
    "                True,                             # rsvp = True\n",
    "                rsvp_row[approved_col],          # approved?\n",
    "                checked_in,                           # checked_in defaults to False\n",
    "                pd.to_datetime(rsvp_row[rsvp_date_col]),\n",
    "                False,                           # is_first_event\n",
    "                pd.NA,                           # invite_token_id\n",
    "            ]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Process Attendance\n",
    "# -----------------------------------------------------------------------------\n",
    "##########################################################\n",
    "# attendance params\n",
    "##########################################################\n",
    "eid_key = {\n",
    "    \"Launch_Sheets.csv\": 1,\n",
    "    \"Pentera_Sheets.csv\": 8,\n",
    "    \"PreLaunch_Sheets.csv\": 0,\n",
    "    \"Russell_Sheets.csv\": 2,\n",
    "    \"SababaNights_Sheets.csv\": 4,\n",
    "    \"SecretSip_Sheets.csv\": 7,\n",
    "    \"Viale_Sheets.csv\": 3,\n",
    "    \"ZelnickDinner_Sheets.csv\": 11,\n",
    "    \"ZelnickSpeaker_Sheets.csv\": 10,\n",
    "}\n",
    "\n",
    "num_attend_no_rsvp = 0\n",
    "indices_to_handle_attendance = []\n",
    "for attended_csv_file_name in attended_sheets:\n",
    "    eid = eid_key[attended_csv_file_name]\n",
    "    attended_email_col = \"email\"\n",
    "    attended_csv = pd.read_csv(f\"Raw/{attended_csv_file_name}\")\n",
    "\n",
    "    # Split the 'name' column into 'first_name' and 'last_name'\n",
    "    attended_csv[[\"first_name\", \"last_name\"]] = (\n",
    "        attended_csv[\"name\"].fillna(\"\").str.split(\" \", n=1, expand=True)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Handling attendance for {attended_csv_file_name}...\\n\")\n",
    "    for i, att_row in attended_csv.iterrows():\n",
    "        pid, people = find_person_id(\n",
    "            att_row,\n",
    "            people,\n",
    "            contacts,\n",
    "            email_col=attended_email_col,\n",
    "            handle_indices_list=indices_to_handle_attendance,\n",
    "        )\n",
    "        if pid is not None:\n",
    "            # Instead of modifying \"attendance[attendance['person_id'] == pid]['checked_in'] = True\",\n",
    "            # which can generate SettingWithCopyWarning, use 'loc' to update properly:\n",
    "            if attendance.loc[((attendance[\"person_id\"] == pid) & (attendance['event_id'] == eid)), \"checked_in\"].empty:\n",
    "                num_attend_no_rsvp += 1\n",
    "                attendance.loc[len(attendance)] = [\n",
    "                    pid,\n",
    "                    eid,\n",
    "                    True,    # rsvp\n",
    "                    True,    # approved\n",
    "                    True,    # checked_in\n",
    "                    pd.NaT,  # rsvp_datetime\n",
    "                    False,   # is_first_event\n",
    "                    pd.NA,   # invite_token_id\n",
    "                ]\n",
    "            else:\n",
    "                attendance.loc[((attendance[\"person_id\"] == pid) & (attendance['event_id'] == eid)), \"checked_in\"] = True\n",
    "\n",
    "\n",
    "\n",
    "# for these ones, rsvp is done with attendance because there was no rsvp, so we fill the columns as approved\n",
    "eid_key = { \"Amplify_Sheets.csv\": 5,\n",
    "    \"Community Dinners II_Sheets.csv\": 12,\n",
    "    \"CommunityDinners_Sheets.csv\": 6,}\n",
    "indices_to_handle_no_rsvp = []\n",
    "for norsvp_csv_file_name in no_rsvp_sheets:\n",
    "    eid = eid_key[norsvp_csv_file_name]\n",
    "    norsvp_email_col = \"email\"\n",
    "\n",
    "    # Read CSVs\n",
    "    norsvp_csv = pd.read_csv(f\"Raw/{norsvp_csv_file_name}\")\n",
    "    norsvp_csv[[\"first_name\", \"last_name\"]] = (\n",
    "        norsvp_csv[\"name\"].fillna(\"\").str.split(\" \", n=1, expand=True)\n",
    "    )\n",
    "\n",
    "    print(f\"Handling no RSVPs for {norsvp_csv_file_name}...\\n\")\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Process RSVPs\n",
    "    # -----------------------------------------------------------------------------\n",
    "    for i, norsvp_row in norsvp_csv.iterrows():\n",
    "        pid, people = find_person_id(\n",
    "            norsvp_row,\n",
    "            people,\n",
    "            contacts,\n",
    "            email_col=norsvp_email_col,\n",
    "            handle_indices_list=indices_to_handle_no_rsvp,\n",
    "        )\n",
    "        if pid is not None:  # <--- Add this guard\n",
    "            attendance.loc[len(attendance)] = [\n",
    "                pid,\n",
    "                eid,\n",
    "                True,    # rsvp\n",
    "                True,    # approved\n",
    "                True,    # checked_in\n",
    "                pd.NaT,  # rsvp_datetime\n",
    "                False,   # is_first_event\n",
    "                pd.NA,   # invite_token_id\n",
    "            ]\n",
    "\n",
    "# summmary data\n",
    "print(f\"Found {num_attend_no_rsvp} attendees with no RSVP.\")\n",
    "print(f\"Made {len(attendance)} attendance entries.\")\n",
    "print(f\"Found {len(indices_to_handle_rsvp + indices_to_handle_attendance + indices_to_handle_no_rsvp)} rsvp indices to handle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "invite_tokens = pd.DataFrame(columns=[\"id\", \"event_id\", \"category\", \"description\"])\n",
    "for i in range(0, 13):\n",
    "    invite_tokens.loc[len(invite_tokens)] = [i, i, \"personal outreach\", \"before we began tracking this\"]\n",
    "\n",
    "attendance[\"invite_token_id\"] = attendance[\"event_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.to_csv(\"final/people.csv\", index=False)\n",
    "attendance.to_csv(\"final/attendance.csv\")\n",
    "invite_tokens.to_csv(\"final/invite_tokens.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(3),\n",
       " np.int64(4),\n",
       " np.int64(5),\n",
       " np.int64(6),\n",
       " np.int64(7),\n",
       " np.int64(8),\n",
       " np.int64(9),\n",
       " np.int64(10),\n",
       " np.int64(11),\n",
       " np.int64(12)]"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(attendance['event_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "41\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>courtney</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dreese</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eliska</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fernanda</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>francisco</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gabe</td>\n",
       "      <td>evers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>johnson</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kami</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kiran</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nitish</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nolan</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>prash</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>samay</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>saphina</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sreela</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>j.</td>\n",
       "      <td>desire abayo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>james</td>\n",
       "      <td>johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>woodbridge</td>\n",
       "      <td>international society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>giovanni</td>\n",
       "      <td>maria d'antonio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>anil</td>\n",
       "      <td>candilcor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>cybele</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>clyde</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ruby</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>xhech</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>bemnet</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>maggie</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>meera</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>adi</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>elior</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>el</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>dari</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ted</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>adrinao</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>cybele</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    first_name              last_name\n",
       "0                                None\n",
       "1     courtney                   None\n",
       "2       dreese                   None\n",
       "3            e                   None\n",
       "4       eliska                   None\n",
       "5     fernanda                   None\n",
       "6    francisco                   None\n",
       "7         gabe                  evers\n",
       "8      johnson                   None\n",
       "9         kami                   None\n",
       "10       kiran                   None\n",
       "11      nitish                   None\n",
       "12       nolan                   None\n",
       "13       prash                   None\n",
       "14       samay                   None\n",
       "15     saphina                   None\n",
       "16      sreela                   None\n",
       "17          j.           desire abayo\n",
       "18       james                johnson\n",
       "19                                   \n",
       "21  woodbridge  international society\n",
       "23    giovanni        maria d'antonio\n",
       "24        anil              candilcor\n",
       "25      cybele                       \n",
       "26       clyde                   None\n",
       "27           n                   None\n",
       "28        ruby                   None\n",
       "29       xhech                   None\n",
       "30      bemnet                   None\n",
       "32      maggie                   None\n",
       "33       meera                   None\n",
       "34         adi                   None\n",
       "35       elior                   None\n",
       "36          el                   None\n",
       "37        dari                   None\n",
       "38         ted                   None\n",
       "39     adrinao                   None\n",
       "40      cybele                   None"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = len(indices_to_handle_attendance) + len(indices_to_handle_rsvp)\n",
    "# todo, drop duplicates, then add to df\n",
    "new = indices_to_handle_rsvp + indices_to_handle_attendance + indices_to_handle_no_rsvp\n",
    "total2 = len(new)\n",
    "\n",
    "print(total)\n",
    "print(total2)\n",
    "\n",
    "df_additional_names = pd.DataFrame(columns = [\"first_name\", \"last_name\"], data=new)\n",
    "df_additional_names[\"first_name\"] = df_additional_names[\"first_name\"].str.strip().str.lower()\n",
    "df_additional_names[\"last_name\"] = df_additional_names[\"last_name\"].str.strip().str.lower()\n",
    "df_additional_names = df_additional_names.drop_duplicates()\n",
    "df_additional_names.to_csv(\"to_add_people.csv\", index=False)\n",
    "df_additional_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Details To Missing People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Combined 22 raw files into one DataFrame with 3031 total rows.\n",
      "Matching lindsey brail to lindsay brail\n",
      "Matching ana tejeda to ana tejada\n",
      "Matching giovanni d’antonio to giovanni m d’antonio\n",
      "Matching giovanni d’antonio to giovanni m d’antonio\n",
      "Matching giovanni d’antonio to giovanni m d’antonio\n",
      "Matching giovanni d’antonio to giovanni m d’antonio\n",
      "Matching giovanni d’antonio to giovanni m d’antonio\n",
      "Matching giovanni d’antonio to giovanni m d’antonio\n",
      "Matching giovanni d’antonio to giovanni m d’antonio\n",
      "Matching hyunwoo kim to hyunwo kim\n",
      "Matching talia gershon to tali gershon\n",
      "Matching yagmur kavukcuoglu to yagnur kavukcuoglu\n",
      "Matching isabella raskin to isabela raskin\n",
      "Matching asher chamoy to ashey chamoy\n",
      "Matching harley pasternak to harley pasternack\n",
      "Matching harley pasternak to harley pasternack\n",
      "Matching matthew hirschfeld to matthew herschfeld\n",
      "Matching adriano arioto to adriano ariota\n",
      "Matching alexandra otto to alexander otto\n",
      "Matching joshua yang to josh yang\n",
      "Matching giovanni d'antonio to giovanni m d’antonio\n",
      "Matching giovanni d'antonio to giovanni m d’antonio\n",
      "Matching giovanni d'antonio to giovanni m d’antonio\n",
      "Matching giovanni d'antonio to giovanni m d’antonio\n",
      "Matching giovanni d'antonio to giovanni m d’antonio\n",
      "Matching giovanni d'antonio to giovanni m d’antonio\n",
      "Matching giovanni d'antonio to giovanni m d’antonio\n",
      "Matching tali gershon to talia gershon\n",
      "Matching tali gershon to talia gershon\n",
      "Matching tali gershon to talia gershon\n",
      "Matching tali gershon to talia gershon\n",
      "Matching peyton worthington to peyton j worthington\n",
      "Matching ashey chamoy to asher chamoy\n",
      "Matching harley pasternack to harley pasternak\n",
      "Matching harley pasternack to harley pasternak\n",
      "Matching matthew herschfeld to matthew hirschfeld\n",
      "Matching alexander otto to alexandra otto\n",
      "Matching adriano ariota to adriano arioto\n",
      "Matching adriano ariota to adriano arioto\n",
      "Matching isabela raskin to isabella raskin\n",
      "Matching isabela raskin to isabella raskin\n",
      "Matching isabela raskin to isabella raskin\n",
      "Matching yagnur kavukcuoglu to yagmur kavukcuoglu\n",
      "Matching yagnur kavukcuoglu to yagmur kavukcuoglu\n",
      "Matching yagnur kavukcuoglu to yagmur kavukcuoglu\n",
      "Matching yagnur kavukcuoglu to yagmur kavukcuoglu\n",
      "Matching yagnur kavukcuoglu to yagmur kavukcuoglu\n",
      "Matching hyunwo kim to hyunwoo kim\n",
      "Matching hyunwo kim to hyunwoo kim\n",
      "Matching hyunwo kim to hyunwoo kim\n",
      "Matching ana tejada to ana tejeda\n",
      "Matching ana tejada to ana tejeda\n",
      "Matching josh yang to joshua yang\n",
      "Matching josh yang to joshua yang\n",
      "\n",
      "🎉 Final enriched unmatched names saved to 'camel_unmatched_names_with_info.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ===== Load the unmatched names file =====\n",
    "no_match = pd.read_csv(\"camel_unmatched_names.csv\")\n",
    "\n",
    "# Split names into first and last\n",
    "no_match[\"email\"] = [[] for _ in range(len(no_match))]\n",
    "no_match[\"school\"] = [[] for _ in range(len(no_match))]\n",
    "no_match[\"grade\"] = [[] for _ in range(len(no_match))]\n",
    "no_match[\"j/n\"] = [[] for _ in range(len(no_match))]\n",
    "\n",
    "# ===== Step 1: Concat all Raw CSV files into one DataFrame =====\n",
    "all_raw = []\n",
    "for file in os.listdir(\"Raw\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        raw_df = pd.read_csv(f\"Raw/{file}\")\n",
    "\n",
    "        # Split 'name' column into first and last names\n",
    "        if \"name\" in raw_df.columns:\n",
    "            raw_df[['first_name', 'last_name']] = pd.NA\n",
    "            raw_df[['first_name', 'last_name']] = raw_df['name'].str.split(' ', n=1, expand=True)\n",
    "        \n",
    "        # Append to the list\n",
    "        all_raw.append(raw_df)\n",
    "\n",
    "# Combine all raw data into one DataFrame\n",
    "df_all_raw = pd.concat(all_raw, ignore_index=True)\n",
    "print(f\"\\n📊 Combined {len(all_raw)} raw files into one DataFrame with {len(df_all_raw)} total rows.\")\n",
    "\n",
    "# Check available columns\n",
    "search_email = \"email\" in df_all_raw.columns\n",
    "search_school = \"school\" in df_all_raw.columns\n",
    "search_grade = \"grade\" in df_all_raw.columns\n",
    "search_jn = \"j/n\" in df_all_raw.columns\n",
    "\n",
    "# ===== Step 2: Perform matching and information gathering =====\n",
    "\n",
    "# For each unmatched name:\n",
    "for idx_nm, row_nm in no_match.iterrows():\n",
    "    fn_nm = str(row_nm[\"first_name\"]).strip()\n",
    "    ln_nm = str(row_nm[\"last_name\"]).strip()\n",
    "\n",
    "    # Search through all concatenated raw data\n",
    "    for idx_r, row_r in df_all_raw.iterrows():\n",
    "        fn_r = str(row_r[\"first_name\"]).strip() if pd.notna(row_r[\"first_name\"]) else \"\"\n",
    "        ln_r = str(row_r[\"last_name\"]).strip() if pd.notna(row_r[\"last_name\"]) else \"\"\n",
    "\n",
    "        code = compare_names(fn_nm, ln_nm, fn_r, ln_r, fuzzy_threshold=0.80)\n",
    "\n",
    "        if code in [\"auto_accept\", \"manual_review\"]:  # Only consider valid matches\n",
    "            if code == \"manual_review\":\n",
    "                val = input(f\"\"\"\n",
    "                Manual review required for '{fn_nm} {ln_nm}'.\\n \n",
    "                Possible match: {fn_r} {ln_r}\\n\n",
    "                y/n\"\"\").strip().lower()\n",
    "                if val not in [\"yes\", \"y\"]:\n",
    "                    continue\n",
    "            \n",
    "            # Update the unmatched name with the best match\n",
    "            if len(row_nm[\"first_name\"].strip()) < len(row_r[\"first_name\"].strip()):\n",
    "                no_match.at[idx_nm, \"first_name\"] = row_r[\"first_name\"]\n",
    "            if len(row_nm[\"last_name\"].strip()) < len(row_r[\"last_name\"].strip()):\n",
    "                no_match.at[idx_nm, \"last_name\"] = row_r[\"last_name\"]\n",
    "\n",
    "            # For each piece of info, append if valid and not already in list\n",
    "            if search_email:\n",
    "                email = row_r[\"email\"] if pd.notna(row_r[\"email\"]) and str(row_r[\"email\"]).strip().lower() else None\n",
    "                if email and email not in no_match.at[idx_nm, \"email\"]:\n",
    "                    no_match.at[idx_nm, \"email\"].append(email)\n",
    "            \n",
    "            if search_school:\n",
    "                school = row_r[\"school\"] if pd.notna(row_r[\"school\"]) and str(row_r[\"school\"]).strip().lower() else None\n",
    "                if school and school not in no_match.at[idx_nm, \"school\"]:\n",
    "                    no_match.at[idx_nm, \"school\"].append(school)\n",
    "\n",
    "            if search_grade:\n",
    "                grade = row_r[\"grade\"] if pd.notna(row_r[\"grade\"]) and str(row_r[\"grade\"]).strip().lower() else None\n",
    "                if grade and grade not in no_match.at[idx_nm, \"grade\"]:\n",
    "                    no_match.at[idx_nm, \"grade\"].append(grade)\n",
    "\n",
    "            if search_jn:\n",
    "                jn = row_r[\"j/n\"] if pd.notna(row_r[\"j/n\"]) and str(row_r[\"j/n\"]).strip().lower() else None\n",
    "                if jn and jn not in no_match.at[idx_nm, \"j/n\"]:\n",
    "                    no_match.at[idx_nm, \"j/n\"].append(jn)\n",
    "\n",
    "# ===== Step 3: Final formatting to make lists readable (join them) =====\n",
    "\n",
    "for col in [\"email\", \"school\", \"grade\", \"j/n\"]:\n",
    "    no_match[col] = no_match[col].apply(lambda x: ', '.join(map(str, x)) if x else pd.NA)\n",
    "\n",
    "# ===== Step 4: Save the enhanced unmatched names file =====\n",
    "no_match.to_csv(\"camel_unmatched_names_with_info.csv\", index=False)\n",
    "\n",
    "print(\"\\n🎉 Final enriched unmatched names saved to 'camel_unmatched_names_with_info.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Found duplicate candidates:\n",
      "  1) nan nan\n",
      "  2) nan nan\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) nan nan\n",
      "  2) nan nan\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) nan nan\n",
      "  2) nan nan\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) nan nan\n",
      "  2) nan nan\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) nan nan\n",
      "  2) nan nan\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) nan nan\n",
      "  2) nan nan\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) nan nan\n",
      "  2) nan nan\n",
      " -> Merging and keeping first.\n",
      "Matching ana tejeda to ana tejada\n",
      "🔗 Found duplicate candidates:\n",
      "  1) ana tejeda\n",
      "  2) ana tejada\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) cami kotouc\n",
      "  2) cami Kotouc\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) christina Lee\n",
      "  2) christina lee\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) connor Kim\n",
      "  2) connor kim\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) elior nan\n",
      "  2) elior nan\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) giovanni M D’Antonio\n",
      "  2) giovanni M D’Antonio\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) hyunwoo kim\n",
      "  2) Hyunwoo kim\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) sam mitchell\n",
      "  2) sam Mitchell\n",
      " -> Merging and keeping first.\n",
      "Matching samay nan to sammy nan\n",
      "🔗 Found duplicate candidates:\n",
      "  1) samay nan\n",
      "  2) sammy nan\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) talia gershon\n",
      "  2) Talia gershon\n",
      " -> Merging and keeping first.\n",
      "Matching yagmur kavukcuoglu to yagnur kavukcuoglu\n",
      "🔗 Found duplicate candidates:\n",
      "  1) yagmur kavukcuoglu\n",
      "  2) yagnur kavukcuoglu\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) isabella raskin\n",
      "  2) Isabella raskin\n",
      " -> Merging and keeping first.\n",
      "Matching asher chamoy to ashey chamoy\n",
      "🔗 Found duplicate candidates:\n",
      "  1) asher chamoy\n",
      "  2) ashey chamoy\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) harley Pasternack\n",
      "  2) harley pasternack\n",
      " -> Merging and keeping first.\n",
      "Matching matthew hirschfeld to matthew herschfeld\n",
      "🔗 Found duplicate candidates:\n",
      "  1) matthew hirschfeld\n",
      "  2) matthew herschfeld\n",
      " -> Merging and keeping first.\n",
      "Matching adriano arioto to adriano ariota\n",
      "🔗 Found duplicate candidates:\n",
      "  1) adriano arioto\n",
      "  2) adriano ariota\n",
      " -> Merging and keeping first.\n",
      "Matching alexandra otto to alexander otto\n",
      "🔗 Found duplicate candidates:\n",
      "  1) alexandra otto\n",
      "  2) alexander otto\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) joseph abayo\n",
      "  2) joseph abayo\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) joshua yang\n",
      "  2) Joshua yang\n",
      " -> Merging and keeping first.\n",
      "🔗 Found duplicate candidates:\n",
      "  1) nicole nan\n",
      "  2) nicole nan\n",
      " -> Merging and keeping first.\n",
      "\n",
      "✅ Deduplicated file saved as 'camel_unmatched_names_deduplicated.csv'. 27 duplicates removed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load enhanced unmatched names file\n",
    "df = pd.read_csv(\"camel_unmatched_names_with_info.csv\")\n",
    "\n",
    "# Ensure lists for merging\n",
    "for col in [\"email\", \"school\", \"grade\", \"j/n\"]:\n",
    "    df[col] = df[col].fillna('').apply(lambda x: [item.strip() for item in x.split(\",\") if item.strip()] if x else [])\n",
    "\n",
    "# Track indices to remove (duplicates)\n",
    "rows_to_remove = set()\n",
    "\n",
    "# Iterate over each pair of rows (avoiding redundant pairs and self-pair)\n",
    "for idx_a in range(len(df)):\n",
    "    if idx_a in rows_to_remove:\n",
    "        continue  # skip rows already marked for deletion\n",
    "\n",
    "    fn_a = str(df.at[idx_a, \"first_name\"]).strip()\n",
    "    ln_a = str(df.at[idx_a, \"last_name\"]).strip()\n",
    "\n",
    "    for idx_b in range(idx_a + 1, len(df)):\n",
    "        if idx_b in rows_to_remove:\n",
    "            continue  # skip if already marked\n",
    "\n",
    "        fn_b = str(df.at[idx_b, \"first_name\"]).strip()\n",
    "        ln_b = str(df.at[idx_b, \"last_name\"]).strip()\n",
    "\n",
    "        # Use compare_names function with a good threshold\n",
    "        code = compare_names(fn_a, ln_a, fn_b, ln_b, fuzzy_threshold=0.80)\n",
    "\n",
    "        if code in [\"auto_accept\", \"manual_review\"]:\n",
    "            # take the longer first name\n",
    "            if len(fn_a) < len(fn_b):\n",
    "                fn_a = fn_b\n",
    "            if len(ln_a) < len(ln_b):\n",
    "                ln_a = ln_b\n",
    "\n",
    "            # set the first name and last name to the longer one\n",
    "            df.at[idx_a, \"first_name\"] = fn_a\n",
    "            df.at[idx_a, \"last_name\"] = ln_a\n",
    "\n",
    "            print(f\"🔗 Found duplicate candidates:\\n  1) {fn_a} {ln_a}\\n  2) {fn_b} {ln_b}\\n -> Merging and keeping first.\")\n",
    "\n",
    "            # Merge each column's lists\n",
    "            for col in [\"email\", \"school\", \"grade\", \"j/n\"]:\n",
    "                # Union of both lists, avoiding empty strings\n",
    "                combined = set(df.at[idx_a, col]) | set(df.at[idx_b, col])\n",
    "                combined = {x for x in combined if x.strip()}\n",
    "                df.at[idx_a, col] = list(combined)\n",
    "\n",
    "            \n",
    "\n",
    "            # Mark idx_b for removal (since merged into idx_a)\n",
    "            rows_to_remove.add(idx_b)\n",
    "\n",
    "# After merging, convert lists back to comma-separated strings\n",
    "for col in [\"email\", \"school\", \"grade\", \"j/n\"]:\n",
    "    df[col] = df[col].apply(lambda x: ', '.join(sorted(set(x))) if isinstance(x, list) else pd.NA)\n",
    "\n",
    "# Drop duplicate rows\n",
    "df_cleaned = df.drop(index=rows_to_remove).reset_index(drop=True)\n",
    "\n",
    "# Save cleaned version\n",
    "df_cleaned.to_csv(\"camel_unmatched_names_deduplicated.csv\", index=False)\n",
    "print(f\"\\n✅ Deduplicated file saved as 'camel_unmatched_names_deduplicated.csv'. {len(rows_to_remove)} duplicates removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Fully resolved dataset saved to 'camel_unmatched_names_resolved_interactive.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned and deduplicated DataFrame\n",
    "df_cleaned = pd.read_csv(\"camel_unmatched_names_deduplicated.csv\")\n",
    "\n",
    "# Initialize new columns for split emails\n",
    "df_cleaned['personal_email'] = pd.NA\n",
    "df_cleaned['school_email'] = pd.NA\n",
    "\n",
    "# Function to process and interactively resolve emails with all context in input()\n",
    "def process_emails_interactive(email_str, index, row):\n",
    "    if pd.isna(email_str) or not str(email_str).strip():\n",
    "        return pd.NA, pd.NA\n",
    "\n",
    "    emails = [e.strip().lower() for e in str(email_str).split(\",\") if e.strip()]\n",
    "    emails = list(set(emails))  # Deduplicate\n",
    "    personal_emails = [e for e in emails if not e.endswith('.edu')]\n",
    "    school_emails = [e for e in emails if e.endswith('.edu')]\n",
    "\n",
    "    # Handle personal emails\n",
    "    if len(personal_emails) > 1:\n",
    "        selected_personal = input(\n",
    "            f\"\\n⚠️ Multiple personal emails for '{row['first_name']} {row['last_name']}' (Row {index}): {personal_emails}\\n\"\n",
    "            f\"All emails: {emails}\\n\"\n",
    "            f\"School: {row['school']} | Grade: {row['grade']} | J/N: {row['j/n']}\\n\"\n",
    "            \"Select one personal email (or type full preferred email): \"\n",
    "        ).strip()\n",
    "        personal_emails = [selected_personal] if selected_personal else [personal_emails[0]]\n",
    "    elif len(personal_emails) == 1:\n",
    "        selected_personal = personal_emails[0]\n",
    "    else:\n",
    "        selected_personal = pd.NA\n",
    "\n",
    "    # Handle school emails\n",
    "    if len(school_emails) > 1:\n",
    "        selected_school = input(\n",
    "            f\"\\n⚠️ Multiple school emails for '{row['first_name']} {row['last_name']}' (Row {index}): {school_emails}\\n\"\n",
    "            f\"All emails: {emails}\\n\"\n",
    "            f\"School: {row['school']} | Grade: {row['grade']} | J/N: {row['j/n']}\\n\"\n",
    "            \"Select one school email (or type full preferred email): \"\n",
    "        ).strip()\n",
    "        school_emails = [selected_school] if selected_school else [school_emails[0]]\n",
    "    elif len(school_emails) == 1:\n",
    "        selected_school = school_emails[0]\n",
    "    else:\n",
    "        selected_school = pd.NA\n",
    "\n",
    "    return selected_personal, selected_school\n",
    "\n",
    "# Function to resolve other fields interactively with all context in input()\n",
    "def resolve_field_interactive(value_str, column_name, index, row):\n",
    "    if pd.isna(value_str) or not str(value_str).strip():\n",
    "        return pd.NA\n",
    "    values = [v.strip() for v in str(value_str).split(\",\") if v.strip()]\n",
    "    if column_name == \"school\":\n",
    "        if len(values) > 1 and \"Other\" in values or \"other\" in values:\n",
    "            values = [v for v in values if v.lower() != \"other\"]\n",
    "    if len(values) > 1:\n",
    "        selected_value = input(\n",
    "            f\"\\n⚠️ Multiple values for '{column_name}' in '{row['first_name']} {row['last_name']}' (Row {index}): {values}\\n\"\n",
    "            f\"Emails: {row['email']} | School: {row['school']} | Grade: {row['grade']} | J/N: {row['j/n']}\\n\"\n",
    "            f\"Select one for '{column_name}' (or type correct value): \"\n",
    "        ).strip()\n",
    "        return selected_value if selected_value else values[0]\n",
    "    else:\n",
    "        return values[0] if values else pd.NA\n",
    "\n",
    "# ===== Process each row with full-context interactive resolution =====\n",
    "for idx, row in df_cleaned.iterrows():\n",
    "    # Resolve email fields with context in input()\n",
    "    personal_email, school_email = process_emails_interactive(row['email'], idx, row)\n",
    "    df_cleaned.at[idx, 'personal_email'] = personal_email\n",
    "    df_cleaned.at[idx, 'school_email'] = school_email\n",
    "\n",
    "    # Resolve other fields interactively with context in input()\n",
    "    df_cleaned.at[idx, 'school'] = resolve_field_interactive(row['school'], 'school', idx, row)\n",
    "    df_cleaned.at[idx, 'grade'] = resolve_field_interactive(row['grade'], 'grade', idx, row)\n",
    "    df_cleaned.at[idx, 'j/n'] = resolve_field_interactive(row['j/n'], 'j/n', idx, row)\n",
    "\n",
    "# ===== Optional: Drop old email column if no longer needed =====\n",
    "df_cleaned.drop(columns=['email'], inplace=True)\n",
    "\n",
    "# ===== Save fully resolved dataset =====\n",
    "df_cleaned.to_csv(\"camel_unmatched_names_resolved_interactive.csv\", index=False)\n",
    "print(\"\\n✅ Fully resolved dataset saved to 'camel_unmatched_names_resolved_interactive.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705\n",
      "1283\n",
      "1251\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.columns = [\"first_name\", \"last_name\", \"School\", \"Grade\", \"J/N\", \"Emails\", \"School Email\"]\n",
    "df_cleaned\n",
    "\n",
    "df_all_people = pd.read_csv(\"all_people.csv\")\n",
    "t1 = len(df_all_people)\n",
    "\n",
    "t2 = len(pd.concat([df_all_people, df_cleaned]))\n",
    "\n",
    "df_all_people = pd.concat([df_all_people, df_cleaned])\n",
    "df_all_people.drop_duplicates(subset=[\"first_name\", \"last_name\"], inplace=True)\n",
    "t3 = len(df_all_people)\n",
    "df_all_people.to_csv(\"all_people2.csv\", index=False)\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Found duplicate candidates:\n",
      "  1) Rowan gupta\n",
      "  2) rowan gupta\n",
      " -> Merging and keeping first.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load enhanced unmatched names file\n",
    "df = pd.read_csv(\"all_people2.csv\")\n",
    "\n",
    "# Ensure lists for merging\n",
    "for col in [\"School\", \"Grade\", \"J/N\", \"Emails\", \"School Email\"]:\n",
    "    df[col] = df[col].fillna('').apply(lambda x: [item.strip() for item in x.split(\",\") if item.strip()] if x else [])\n",
    "\n",
    "# Track indices to remove (duplicates)\n",
    "rows_to_remove = set()\n",
    "\n",
    "# Iterate over each pair of rows (avoiding redundant pairs and self-pair)\n",
    "for idx_a in range(len(df)):\n",
    "    if idx_a in rows_to_remove:\n",
    "        continue  # skip rows already marked for deletion\n",
    "\n",
    "    fn_a = str(df.at[idx_a, \"first_name\"]).strip()\n",
    "    ln_a = str(df.at[idx_a, \"last_name\"]).strip()\n",
    "\n",
    "    for idx_b in range(idx_a + 1, len(df)):\n",
    "        if idx_b in rows_to_remove:\n",
    "            continue  # skip if already marked\n",
    "\n",
    "        fn_b = str(df.at[idx_b, \"first_name\"]).strip()\n",
    "        ln_b = str(df.at[idx_b, \"last_name\"]).strip()\n",
    "\n",
    "        # Use compare_names function with a good threshold\n",
    "        code = compare_names(fn_a, ln_a, fn_b, ln_b, fuzzy_threshold=0.80)\n",
    "\n",
    "        if code in [\"auto_accept\", \"manual_review\"]:\n",
    "            # take the longer first name\n",
    "            if len(fn_a) < len(fn_b):\n",
    "                fn_a = fn_b\n",
    "            if len(ln_a) < len(ln_b):\n",
    "                ln_a = ln_b\n",
    "\n",
    "            # set the first name and last name to the longer one\n",
    "            df.at[idx_a, \"first_name\"] = fn_a\n",
    "            df.at[idx_a, \"last_name\"] = ln_a\n",
    "\n",
    "            print(f\"🔗 Found duplicate candidates:\\n  1) {fn_a} {ln_a}\\n  2) {fn_b} {ln_b}\\n -> Merging and keeping first.\")\n",
    "\n",
    "            # Merge each column's lists\n",
    "            for col in [\"School\", \"Grade\", \"J/N\", \"Emails\", \"School Email\"]:\n",
    "                # Union of both lists, avoiding empty strings\n",
    "                combined = set(df.at[idx_a, col]) | set(df.at[idx_b, col])\n",
    "                combined = {x for x in combined if x.strip()}\n",
    "                df.at[idx_a, col] = list(combined)\n",
    "\n",
    "            \n",
    "\n",
    "            # Mark idx_b for removal (since merged into idx_a)\n",
    "            rows_to_remove.add(idx_b)\n",
    "\n",
    "# After merging, convert lists back to comma-separated strings\n",
    "for col in [\"School\", \"Grade\", \"J/N\", \"Emails\", \"School Email\"]:\n",
    "    df[col] = df[col].apply(lambda x: ', '.join(sorted(set(x))) if isinstance(x, list) else pd.NA)\n",
    "\n",
    "# Drop duplicate rows\n",
    "df_cleaned = df.drop(index=rows_to_remove).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "import pandas as pd\n",
    "filled = pd.read_csv(\"manual_filled.csv\")\n",
    "for index, row in filled.iterrows():\n",
    "    # set person demographics = to filled in demographics unless None, NA, or \"\"\n",
    "    # if there is a contact value, check if it exists in our db and that it is linked to the current person\n",
    "    # if it exists but is linked to a different person, print the person id, the contact id, and the associated person id\n",
    "    # if it doesn't exist, create a new contact and link it to the current person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "people = pd.read_csv(\"final/people.csv\")\n",
    "contacts = pd.read_csv(\"final/contacts.csv\")\n",
    "\n",
    "# contacts onto to people m:1\n",
    "# create one row per person with contacts as a list of the contact_value column values\n",
    "contacts_grouped = contacts[contacts['contact_type'] == \"school email\"].groupby(\"person_id\")[\"contact_value\"].first().reset_index()\n",
    "people = people.merge(contacts_grouped, left_on=\"id\", right_on=\"person_id\", how=\"left\")\n",
    "people.rename(columns={\"contact_value\": \"contacts\"}, inplace=True)\n",
    "people.drop(columns=[\"person_id\", \"preferred_name\"], inplace=True)\n",
    "\n",
    "# find any null values unless school = Other, then class year can be null and school email can be null\n",
    "to_fill = people[people[\"school\"] != \"Other\"]\n",
    "to_fill = to_fill[to_fill.isna().any(axis=1)]\n",
    "to_fill_other = people[people[\"school\"] == \"Other\"]\n",
    "to_fill_other = to_fill_other[to_fill_other[[\"first_name\", \"last_name\", \"gender\", \"is_jewish\", \"school\"]].isna().any(axis=1)]\n",
    "\n",
    "to_fill = pd.concat([to_fill, to_fill_other])\n",
    "to_fill.drop_duplicates(inplace=True)\n",
    "\n",
    "to_fill[\"class_year\"] = to_fill[\"class_year\"].apply(lambda x: pd.NA if pd.isna(x) else int(x))\n",
    "to_fill.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fill.to_csv(\"to_fill.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>class_year</th>\n",
       "      <th>is_jewish</th>\n",
       "      <th>school</th>\n",
       "      <th>contacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>josie</td>\n",
       "      <td>whelan</td>\n",
       "      <td>F</td>\n",
       "      <td>2028</td>\n",
       "      <td></td>\n",
       "      <td>Harvard</td>\n",
       "      <td>Josiewhelan@college.harvard.edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>christian</td>\n",
       "      <td>armaly</td>\n",
       "      <td>M</td>\n",
       "      <td>2028</td>\n",
       "      <td></td>\n",
       "      <td>Harvard</td>\n",
       "      <td>christianarmaly@college.harvard.edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>alison</td>\n",
       "      <td>chan</td>\n",
       "      <td>F</td>\n",
       "      <td>2025</td>\n",
       "      <td>N</td>\n",
       "      <td>Harvard</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>samuel</td>\n",
       "      <td>coopersmith</td>\n",
       "      <td>M</td>\n",
       "      <td>2028</td>\n",
       "      <td>N</td>\n",
       "      <td>Harvard</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>eli</td>\n",
       "      <td>solomon</td>\n",
       "      <td>M</td>\n",
       "      <td>2028</td>\n",
       "      <td>J</td>\n",
       "      <td>Harvard</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>1163</td>\n",
       "      <td>danielle</td>\n",
       "      <td>frankel</td>\n",
       "      <td></td>\n",
       "      <td>2026</td>\n",
       "      <td></td>\n",
       "      <td>Other</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>1165</td>\n",
       "      <td>devorah</td>\n",
       "      <td>feder</td>\n",
       "      <td></td>\n",
       "      <td>2026</td>\n",
       "      <td></td>\n",
       "      <td>Other</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>1188</td>\n",
       "      <td>naveh</td>\n",
       "      <td>talmon chvaicer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>J</td>\n",
       "      <td>Other</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>1189</td>\n",
       "      <td>liz</td>\n",
       "      <td>vermeulen</td>\n",
       "      <td></td>\n",
       "      <td>2026</td>\n",
       "      <td>N</td>\n",
       "      <td>Other</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>1190</td>\n",
       "      <td>jodie</td>\n",
       "      <td>shin</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Other</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>811 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id first_name        last_name gender class_year is_jewish   school  \\\n",
       "19      19      josie           whelan      F       2028            Harvard   \n",
       "22      22  christian           armaly      M       2028            Harvard   \n",
       "27      27     alison             chan      F       2025         N  Harvard   \n",
       "42      42     samuel      coopersmith      M       2028         N  Harvard   \n",
       "46      46        eli          solomon      M       2028         J  Harvard   \n",
       "...    ...        ...              ...    ...        ...       ...      ...   \n",
       "1161  1163   danielle          frankel              2026              Other   \n",
       "1163  1165    devorah            feder              2026              Other   \n",
       "1183  1188      naveh  talmon chvaicer                           J    Other   \n",
       "1184  1189        liz        vermeulen              2026         N    Other   \n",
       "1185  1190      jodie             shin                                Other   \n",
       "\n",
       "                                 contacts  \n",
       "19        Josiewhelan@college.harvard.edu  \n",
       "22    christianarmaly@college.harvard.edu  \n",
       "27                                         \n",
       "42                                         \n",
       "46                                         \n",
       "...                                   ...  \n",
       "1161                                       \n",
       "1163                                       \n",
       "1183                                       \n",
       "1184                                       \n",
       "1185                                       \n",
       "\n",
       "[811 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# add is first event\n",
    "import pandas as pd\n",
    "\n",
    "# Load RSVPs CSV\n",
    "attendance = pd.read_csv('final/attendance.csv').iloc[:, 1:]\n",
    "attendance[\"approved\"] = attendance[\"approved\"].astype(bool)\n",
    "# Load Events CSV\n",
    "events = pd.read_csv('final/events.csv')\n",
    "\n",
    "# Add is_first_event column to attendance\n",
    "attendance = attendance.merge(events[['id', 'start_datetime']], left_on='event_id', right_on='id', how='left')\n",
    "\n",
    "# Drop the duplicate 'id' column from events table\n",
    "attendance.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Convert start_datetime to datetime type\n",
    "attendance['start_datetime'] = pd.to_datetime(attendance['start_datetime'])\n",
    "\n",
    "# Sort RSVPs by person_id and event start time\n",
    "attendance_sorted = attendance.sort_values(by=['person_id', 'start_datetime'])\n",
    "first_event_times = attendance_sorted[attendance_sorted['checked_in'] == True]\n",
    "# For each person_id, get the earliest start_datetime (unique pair)\n",
    "first_event_times = first_event_times.groupby('person_id', as_index=False)['start_datetime'].min()\n",
    "\n",
    "\n",
    "attendance_sorted['key'] = attendance_sorted['person_id'].astype(str) + attendance_sorted['start_datetime'].astype(str)\n",
    "first_event_times['key'] = first_event_times['person_id'].astype(str) + first_event_times['start_datetime'].astype(str)\n",
    "# Set is_first_event = True if this is the earliest event, else False\n",
    "attendance_sorted['is_first_event'] = attendance_sorted['key'].isin(first_event_times['key'])\n",
    "attendance_sorted.drop(columns=['key'], inplace=True)\n",
    "attendance_sorted.sort_values(by=['start_datetime'], inplace=True)\n",
    "attendance_sorted.drop(columns=['start_datetime'], inplace=True)\n",
    "\n",
    "attendance = attendance_sorted\n",
    "\n",
    "events = pd.read_csv(\"final/events.csv\")\n",
    "people = pd.read_csv(\"final/people.csv\")\n",
    "\n",
    "attendance = attendance.merge(people, left_on=\"person_id\", right_on=\"id\", how=\"left\")\n",
    "\n",
    "# Standardize data\n",
    "attendance[\"is_jewish\"] = attendance[\"is_jewish\"].apply(lambda x: x if pd.isna(x) else x.upper())\n",
    "attendance[\"gender\"] = attendance[\"gender\"].apply(lambda x: x if pd.isna(x) else x.upper())\n",
    "\n",
    "# Aggregate RSVP, Approved, Checked-in counts by event\n",
    "agg_attendance = attendance.groupby(\"event_id\").agg({\n",
    "    \"rsvp\": \"sum\",\n",
    "    \"approved\": \"sum\",\n",
    "    \"checked_in\": \"sum\",\n",
    "    \"is_first_event\": \"sum\"\n",
    "}).reset_index()\n",
    "\n",
    "attendance['is_jewish'] = attendance['is_jewish'].fillna(\"N/A\")\n",
    "attendance['gender'] = attendance['gender'].fillna(\"N/A\")\n",
    "attendance['class_year'] = attendance['class_year'].fillna(1000)\n",
    "# Calculate Jewish status percentages by event\n",
    "jewish_counts = attendance.pivot_table(index=\"event_id\", columns=\"is_jewish\", aggfunc=\"size\")\n",
    "jewish_percentages = jewish_counts.div(jewish_counts.sum(axis=1), axis=0).reset_index()\n",
    "\n",
    "# Calculate Gender percentages by event\n",
    "gender_counts = attendance.pivot_table(index=\"event_id\", columns=\"gender\", aggfunc=\"size\")\n",
    "gender_percentages = gender_counts.div(gender_counts.sum(axis=1), axis=0).reset_index()\n",
    "\n",
    "# Calculate Class year percentages by event\n",
    "class_year_counts = attendance.pivot_table(index=\"event_id\", columns=\"class_year\", aggfunc=\"size\")\n",
    "class_year_percentages = class_year_counts.div(class_year_counts.sum(axis=1), axis=0).reset_index()\n",
    "\n",
    "# Combine all summaries\n",
    "summary = agg_attendance \\\n",
    "    .merge(jewish_percentages, on=\"event_id\", how=\"left\", suffixes=(None, '_jewish')) \\\n",
    "    .merge(gender_percentages, on=\"event_id\", how=\"left\", suffixes=(None, '_gender')) \\\n",
    "    .merge(class_year_percentages, on=\"event_id\", how=\"left\", suffixes=(None, '_class_year'))\n",
    "\n",
    "# Merge with event details (assuming you need event code)\n",
    "final_summary = summary.merge(events[[\"id\", \"category\", \"description\"]], left_on=\"event_id\", right_on=\"id\", how=\"left\")\n",
    "\n",
    "# Drop redundant 'id' column after merge\n",
    "final_summary = final_summary.drop(columns=['id'])\n",
    "\n",
    "\n",
    "final_summary[\"J\"] = final_summary[\"J\"].apply(lambda x: f\"{x:.2%}\" if pd.notna(x) else pd.NA)\n",
    "final_summary[\"N\"] = final_summary[\"N\"].apply(lambda x: f\"{x:.2%}\" if pd.notna(x) else pd.NA)\n",
    "final_summary[\"F\"] = final_summary[\"F\"].apply(lambda x: f\"{x:.2%}\" if pd.notna(x) else pd.NA)\n",
    "final_summary[\"M\"] = final_summary[\"M\"].apply(lambda x: f\"{x:.2%}\" if pd.notna(x) else pd.NA)\n",
    "final_summary[2025.0] = final_summary[2025.0].apply(lambda x: f\"{x:.2%}\" if pd.notna(x) else pd.NA)\n",
    "final_summary[2026.0] = final_summary[2026.0].apply(lambda x: f\"{x:.2%}\" if pd.notna(x) else pd.NA)\n",
    "final_summary[2027.0] = final_summary[2027.0].apply(lambda x: f\"{x:.2%}\" if pd.notna(x) else pd.NA)\n",
    "final_summary[2028.0] = final_summary[2028.0].apply(lambda x: f\"{x:.2%}\" if pd.notna(x) else pd.NA)\n",
    "final_summary.drop(columns=[\"event_id\"], inplace=True)\n",
    "final_summary.drop(columns=[\"N/A\", \"N/A_gender\", 1000.0], inplace=True)\n",
    "final_summary = final_summary[[\"category\", \"description\", \"rsvp\", \"approved\", \"checked_in\", \"is_first_event\", \"J\", \"N\", \"F\", \"M\", 2025.0, 2026.0, 2027.0, 2028.0]]\n",
    "final_summary.to_excel(\"final_summary.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
